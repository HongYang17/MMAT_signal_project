{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457c871f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T02:51:16.356522Z",
     "start_time": "2025-07-04T02:51:15.374798Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import talib\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87f5cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T02:51:16.820238Z",
     "start_time": "2025-07-04T02:51:16.431765Z"
    }
   },
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "sys.path.append(os.path.abspath(\"..\"))  # root /PycharmProjects/MMAT\n",
    "from config.load_env import load_keys\n",
    "\n",
    "keys = load_keys()\n",
    "#print(\"Loaded keys:\", keys)\n",
    "client = Client(keys['api_key'], keys['secret_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4699bb84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T02:51:16.837119Z",
     "start_time": "2025-07-04T02:51:16.827993Z"
    }
   },
   "outputs": [],
   "source": [
    "# ================== Enhanced Feature Engineering ==================\n",
    "class AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lookback_periods=[5, 10, 20, 50]):\n",
    "        self.lookback_periods = lookback_periods\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Ensure index is datetime\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # Basic price features\n",
    "        df['price_change'] = df['close'].pct_change()\n",
    "        df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "        df['close_open_ratio'] = (df['close'] - df['open']) / df['open']\n",
    "\n",
    "        # Multi-timeframe features (e.g., 15min, 30min, 1h)\n",
    "        for period in [15, 30, 60]:\n",
    "            df[f'ema_{period}'] = talib.EMA(df['close'], timeperiod=period)\n",
    "            df[f'sma_{period}'] = talib.SMA(df['close'], timeperiod=period)\n",
    "            df[f'ma_cross_{period}'] = np.where(df[f'ema_{period}'] > df[f'sma_{period}'], 1, -1)\n",
    "\n",
    "        # Volatility features\n",
    "        df['atr'] = talib.ATR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "        df['natr'] = talib.NATR(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "        df['volatility'] = df['close'].rolling(20).std() / df['close'].rolling(20).mean()\n",
    "\n",
    "        # Volume features\n",
    "        df['volume_ma'] = df['volume'].rolling(20).mean()\n",
    "        df['volume_ratio'] = df['volume'] / df['volume_ma']\n",
    "        df['obv'] = talib.OBV(df['close'], df['volume'])\n",
    "\n",
    "        # Momentum features\n",
    "        df['rsi'] = talib.RSI(df['close'], timeperiod=14)\n",
    "        df['macd'], df['macd_signal'], _ = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "        df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "        df['stoch_k'], df['stoch_d'] = talib.STOCH(\n",
    "            df['high'], df['low'], df['close'], fastk_period=14, slowk_period=3, slowd_period=3\n",
    "        )\n",
    "\n",
    "        # Advanced indicators\n",
    "        df['adx'] = talib.ADX(df['high'], df['low'], df['close'], timeperiod=14)\n",
    "        df['cci'] = talib.CCI(df['high'], df['low'], df['close'], timeperiod=20)\n",
    "        df['mfi'] = talib.MFI(df['high'], df['low'], df['close'], df['volume'], timeperiod=14)\n",
    "\n",
    "        # Price pattern features\n",
    "        df['inside_bar'] = ((df['high'] < df['high'].shift(1)) & (df['low'] > df['low'].shift(1))).astype(int)\n",
    "        df['outside_bar'] = ((df['high'] > df['high'].shift(1)) & (df['low'] < df['low'].shift(1))).astype(int)\n",
    "\n",
    "        # Time features\n",
    "        df['hour'] = df.index.hour\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Lag features\n",
    "        for lag in [1, 2, 3, 5]:\n",
    "            df[f'return_lag{lag}'] = df['price_change'].shift(lag)\n",
    "            df[f'volume_ratio_lag{lag}'] = df['volume_ratio'].shift(lag)\n",
    "\n",
    "        # Target encoding features (omitted here to avoid lookahead bias in real applications)\n",
    "\n",
    "        # Remove rows with missing values\n",
    "        df = df.dropna()\n",
    "\n",
    "        return df\n",
    "\n",
    "# ================== Meta-Learning Model Architecture ==================\n",
    "# class MetaModel:\n",
    "#     def __init__(self):\n",
    "#         # Define base learners\n",
    "#         self.base_models = [\n",
    "#             ('xgb_base', XGBClassifier(\n",
    "#                 n_estimators=300,\n",
    "#                 max_depth=5,\n",
    "#                 learning_rate=0.05,\n",
    "#                 subsample=0.8,\n",
    "#                 colsample_bytree=0.8,\n",
    "#                 gamma=0.1,\n",
    "#                 reg_alpha=0.1,\n",
    "#                 reg_lambda=0.1,\n",
    "#                 objective='multi:softprob',\n",
    "#                 num_class=3,\n",
    "#                 use_label_encoder=False,\n",
    "#                 eval_metric='mlogloss'\n",
    "#             )),\n",
    "#             ('rf', RandomForestClassifier(\n",
    "#                 n_estimators=200,\n",
    "#                 max_depth=7,\n",
    "#                 class_weight='balanced',\n",
    "#                 random_state=42\n",
    "#             )),\n",
    "#             ('gbm', GradientBoostingClassifier(\n",
    "#                 n_estimators=150,\n",
    "#                 learning_rate=0.05,\n",
    "#                 max_depth=4,\n",
    "#                 random_state=42\n",
    "#             ))\n",
    "#         ]\n",
    "#\n",
    "#         # Meta-level learner (stacking)\n",
    "#         self.meta_model = XGBClassifier(\n",
    "#             n_estimators=100,\n",
    "#             max_depth=3,\n",
    "#             learning_rate=0.1,\n",
    "#             objective='multi:softprob',\n",
    "#             num_class=3,\n",
    "#             use_label_encoder=False,\n",
    "#             eval_metric='mlogloss'\n",
    "#         )\n",
    "#         self.scaler = StandardScaler()\n",
    "#\n",
    "#     def fit(self, X, y):\n",
    "#         sample_weights = compute_sample_weight(class_weight='balanced', y=y)\n",
    "#         base_preds = []\n",
    "#\n",
    "#         # Train base models and collect predictions\n",
    "#         for name, model in self.base_models:\n",
    "#             model.fit(X, y, sample_weight=sample_weights)\n",
    "#             preds = model.predict_proba(X)\n",
    "#             base_preds.append(preds)\n",
    "#\n",
    "#         # Combine predictions as meta-features\n",
    "#         meta_X = np.hstack(base_preds)\n",
    "#         meta_X_scaled = self.scaler.fit_transform(meta_X)\n",
    "#\n",
    "#         # Train meta-model\n",
    "#         self.meta_model.fit(meta_X_scaled, y, sample_weight=sample_weights)\n",
    "#         self.base_models = [(name, model) for name, model in self.base_models]\n",
    "#\n",
    "#     def predict_proba(self, X):\n",
    "#         base_preds = [model.predict_proba(X) for _, model in self.base_models]\n",
    "#         meta_X = np.hstack(base_preds)\n",
    "#         meta_X_scaled = self.scaler.transform(meta_X)\n",
    "#         return self.meta_model.predict_proba(meta_X_scaled)\n",
    "#\n",
    "#     def predict(self, X):\n",
    "#         proba = self.predict_proba(X)\n",
    "#         # Penalize neutral class, encourage directional signals\n",
    "#         adjusted = proba * np.array([1.2, 0.9, 1.2])  # [down, neutral, up]\n",
    "#         return np.argmax(adjusted, axis=1)\n",
    "#\n",
    "# # ================== Class Balancing ==================\n",
    "# def balance_classes(df, label_col='label'):\n",
    "#     from sklearn.utils import resample\n",
    "#\n",
    "#     # Split by class\n",
    "#     df_neg = df[df[label_col] == -1]\n",
    "#     df_zero = df[df[label_col] == 0]\n",
    "#     df_pos = df[df[label_col] == 1]\n",
    "#\n",
    "#     # Upsample all to the size of the largest class\n",
    "#     max_len = max(len(df_neg), len(df_zero), len(df_pos))\n",
    "#     df_neg_up = resample(df_neg, replace=True, n_samples=max_len, random_state=42)\n",
    "#     df_zero_up = resample(df_zero, replace=True, n_samples=max_len, random_state=42)\n",
    "#     df_pos_up = resample(df_pos, replace=True, n_samples=max_len, random_state=42)\n",
    "#\n",
    "#     # Combine and shuffle\n",
    "#     df_balanced = pd.concat([df_neg_up, df_zero_up, df_pos_up]).sample(frac=1, random_state=42)\n",
    "#     return df_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d989f38619c66b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T02:51:16.872965Z",
     "start_time": "2025-07-04T02:51:16.850698Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, webbrowser\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_pattern_results(df, patterns, symbol,\n",
    "                         max_points=2000, buffer=50,\n",
    "                         open_browser=True):\n",
    "    # 1) slice out the window you care about\n",
    "    start = max(len(df) - max_points - buffer, 0)\n",
    "    dfp = df.iloc[start:].copy()\n",
    "\n",
    "    # 2) ensure output dir\n",
    "    out = '../plots/'\n",
    "    os.makedirs(out, exist_ok=True)\n",
    "\n",
    "    # 3) each pattern → one chart\n",
    "    for name in patterns:\n",
    "        sig = f\"Signal_{name}\"\n",
    "        if sig not in dfp.columns:\n",
    "            print(f\"no column {sig}, skipping\")\n",
    "            continue\n",
    "\n",
    "        up   = dfp[dfp[sig] == 1]\n",
    "        down = dfp[dfp[sig] == -1]\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=1, shared_xaxes=True,\n",
    "            row_heights=[0.7,0.3], vertical_spacing=0.05,\n",
    "            subplot_titles=['Price + Signals','RSI']\n",
    "        )\n",
    "\n",
    "        # candles\n",
    "        fig.add_trace(go.Candlestick(\n",
    "            x=dfp.index, open=dfp['open'], high=dfp['high'],\n",
    "            low=dfp['low'], close=dfp['close'], name='Candles'\n",
    "        ), row=1, col=1)\n",
    "\n",
    "        # MAs\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=dfp.index, y=dfp['MA20'], mode='lines', name='MA20'\n",
    "        ), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=dfp.index, y=dfp['MA50'], mode='lines', name='MA50'\n",
    "        ), row=1, col=1)\n",
    "\n",
    "        # bullish\n",
    "        if not up.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=up.index, y=up['close'] * 1.005,\n",
    "                mode='markers', name='Bullish',\n",
    "                marker=dict(symbol='triangle-up', color='green', size=10)\n",
    "            ), row=1, col=1)\n",
    "\n",
    "        # bearish\n",
    "        if not down.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=down.index, y=down['close'] * 0.995,\n",
    "                mode='markers', name='Bearish',\n",
    "                marker=dict(symbol='triangle-down', color='red', size=10)\n",
    "            ), row=1, col=1)\n",
    "\n",
    "        # RSI subplot\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=dfp.index, y=dfp['RSI'], mode='lines', name='RSI'\n",
    "        ), row=2, col=1)\n",
    "        fig.add_hline(y=50, line_dash='dash', line_color='gray', row=2, col=1)\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{symbol} — {name} Signals\",\n",
    "            xaxis_rangeslider_visible=False,\n",
    "            template='plotly_white',\n",
    "            height=600\n",
    "        )\n",
    "\n",
    "        fn = os.path.join(out, f\"{name}_signals.html\")\n",
    "        fig.write_html(fn)\n",
    "        print(f\"Saved {fn}\")\n",
    "        if open_browser:\n",
    "            webbrowser.open('file://' + os.path.abspath(fn))\n",
    "\n",
    "        # clear the figure before next\n",
    "        fig.data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6cdce1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T02:51:16.992612Z",
     "start_time": "2025-07-04T02:51:16.877889Z"
    }
   },
   "outputs": [],
   "source": [
    "# ================== signal_prediction_pipeline_updated.py ==================\n",
    "# Integrated quant signal prediction pipeline including:\n",
    "# 1. Quantile-based labeling (calculate_target_quantile)\n",
    "# 2. Class balancing (SMOTE)\n",
    "# 3. Block Bootstrap for White’s Reality Check (block_bootstrap_pval)\n",
    "# 4. Weighted performance evaluation (weighted_signal_evaluation)\n",
    "# 5. Candlestick pattern features (integrate_candlestick_features)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import talib\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# === Quantile-based labeling ===\n",
    "def calculate_target_quantile(df, future_bars=3, lower_q=0.2, upper_q=0.8):\n",
    "    future_return = df['close'].shift(-future_bars) / df['close'] - 1\n",
    "    lower = future_return.quantile(lower_q)\n",
    "    upper = future_return.quantile(upper_q)\n",
    "    conds = [\n",
    "        future_return <= lower,\n",
    "        (future_return > lower) & (future_return < upper),\n",
    "        future_return >= upper\n",
    "    ]\n",
    "    labels = [-1, 0, 1]\n",
    "    direction = np.select(conds, labels, default=0)\n",
    "    magnitude = future_return.abs()\n",
    "    return pd.DataFrame({'direction': direction, 'magnitude': magnitude}, index=df.index)\n",
    "\n",
    "# === Simple oversampling (alternative to SMOTE) ===\n",
    "def simple_oversample(X, y):\n",
    "    df_train = X.copy()\n",
    "    df_train['direction'] = y.values\n",
    "    majority = df_train[df_train['direction'] == 0]\n",
    "    minority = df_train[df_train['direction'] != 0]\n",
    "    minority_upsampled = resample(\n",
    "        minority,\n",
    "        replace=True,\n",
    "        n_samples=len(majority),\n",
    "        random_state=42\n",
    "    )\n",
    "    df_bal = pd.concat([majority, minority_upsampled])\n",
    "    y_bal = df_bal['direction']\n",
    "    X_bal = df_bal.drop(columns='direction')\n",
    "    return X_bal, y_bal\n",
    "\n",
    "# === Candlestick pattern features ===\n",
    "def calculate_patterns(df):\n",
    "    funcs = {\n",
    "        'Hammer': talib.CDLHAMMER,\n",
    "        'InvertedHammer': talib.CDLINVERTEDHAMMER,\n",
    "        'BullishEngulfing': lambda o,h,l,c: np.where(talib.CDLENGULFING(o,h,l,c)==100,100,0),\n",
    "        'BearishEngulfing': lambda o,h,l,c: np.where(talib.CDLENGULFING(o,h,l,c)==-100,-100,0),\n",
    "        'PiercingLine': talib.CDLPIERCING,\n",
    "        'DarkCloudCover': talib.CDLDARKCLOUDCOVER,\n",
    "        'MorningStar': talib.CDLMORNINGSTAR,\n",
    "        'EveningStar': talib.CDLEVENINGSTAR,\n",
    "        'ThreeWhiteSoldiers': talib.CDL3WHITESOLDIERS,\n",
    "        'ThreeBlackCrows': talib.CDL3BLACKCROWS,\n",
    "        # Add more patterns as needed\n",
    "    }\n",
    "    for name, fn in funcs.items():\n",
    "        df[name] = fn(df['open'].values, df['high'].values, df['low'].values, df['close'].values)\n",
    "    return df\n",
    "\n",
    "def aggregate_candlestick_signals(df):\n",
    "    bullish = ['Hammer','InvertedHammer','BullishEngulfing','PiercingLine','MorningStar','ThreeWhiteSoldiers']\n",
    "    bearish = ['BearishEngulfing','DarkCloudCover','EveningStar','ThreeBlackCrows']\n",
    "    df['bullish_score'] = df[bullish].eq(100).sum(axis=1)\n",
    "    df['bearish_score'] = df[bearish].eq(-100).sum(axis=1)\n",
    "    df['candlestick_score'] = df['bullish_score'] - df['bearish_score']\n",
    "    return df\n",
    "\n",
    "def integrate_candlestick_features(df):\n",
    "    df = calculate_patterns(df)\n",
    "    df = aggregate_candlestick_signals(df)\n",
    "    return df.dropna()\n",
    "\n",
    "# === Feature engineering ===\n",
    "class AdvancedFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        df['price_change'] = df['close'].pct_change()\n",
    "        df['high_low_ratio'] = (df['high'] - df['low']) / df['close']\n",
    "        df['close_open_ratio'] = (df['close'] - df['open']) / df['open']\n",
    "        df['rsi'] = talib.RSI(df['close'])\n",
    "        df['macd'], df['macd_signal'], _ = talib.MACD(df['close'])\n",
    "        df['volume_ma'] = df['volume'].rolling(20).mean()\n",
    "        df['volume_ratio'] = df['volume'] / df['volume_ma']\n",
    "        df['price_volume_corr'] = df['close'].rolling(20).corr(df['volume'])\n",
    "        df['rsi_divergence'] = df['close'] - df['rsi']\n",
    "        return df.dropna()\n",
    "\n",
    "# === Enhanced MetaModel with PCA + ElasticNet ===\n",
    "class EnhancedMetaModel:\n",
    "    def __init__(self, n_pca=20, l1_ratio=0.5):\n",
    "        self.pca = PCA(n_components=n_pca)\n",
    "        self.base_models = [\n",
    "            ('xgb', XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                reg_alpha=1.0,\n",
    "                reg_lambda=1.0,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='mlogloss'\n",
    "            )),\n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=8,\n",
    "                class_weight='balanced'\n",
    "            )),\n",
    "            ('gbm', GradientBoostingClassifier(\n",
    "                n_estimators=100,\n",
    "                learning_rate=0.1\n",
    "            ))\n",
    "        ]\n",
    "        self.meta_model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(n_components=n_pca)),\n",
    "            ('clf', LogisticRegression(\n",
    "                penalty='elasticnet',\n",
    "                solver='saga',\n",
    "                l1_ratio=l1_ratio,\n",
    "                C=1.0,\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced'\n",
    "            ))\n",
    "        ])\n",
    "        self.scaler = StandardScaler()\n",
    "        self.encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_pca = self.pca.fit_transform(self.scaler.fit_transform(X))\n",
    "        y_enc = self.encoder.fit_transform(y)\n",
    "        meta_X = []\n",
    "        for name, model in self.base_models:\n",
    "            model.fit(X_pca, y_enc)\n",
    "            meta_X.append(model.predict_proba(X_pca))\n",
    "        meta_X = np.hstack(meta_X)\n",
    "        self.meta_model.fit(X, y_enc)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_pca = self.pca.transform(self.scaler.transform(X))\n",
    "        meta_X = np.hstack([m.predict_proba(X_pca) for _, m in self.base_models])\n",
    "        preds = self.meta_model.predict(X)\n",
    "        return self.encoder.inverse_transform(preds)\n",
    "\n",
    "# === Block Bootstrap p-value for White Reality Check ===\n",
    "def block_bootstrap_pval(returns, B=1000, block_len=5, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n = len(returns)\n",
    "    indices = np.arange(n)\n",
    "    boot_means = []\n",
    "    for _ in range(B):\n",
    "        sample_idx = []\n",
    "        nb = int(np.ceil(n / block_len))\n",
    "        for _ in range(nb):\n",
    "            start = np.random.randint(0, n - block_len + 1)\n",
    "            sample_idx.extend(indices[start: start + block_len])\n",
    "        sample_idx = sample_idx[:n]\n",
    "        boot_means.append(returns[sample_idx].mean())\n",
    "    d_bar = returns.mean()\n",
    "    return np.mean([d_bar <= m for m in boot_means])\n",
    "\n",
    "# === Weighted signal evaluation ===\n",
    "def weighted_signal_evaluation(pred_list, y_list, sharpe_list):\n",
    "    weights = np.clip(sharpe_list, 0, None)\n",
    "    weights = weights / weights.sum() if weights.sum() > 0 else np.ones_like(weights) / len(weights)\n",
    "    combined = sum(w * (np.sign(y) * 0.001 * np.sign(p))\n",
    "                   for w, p, y in zip(weights, pred_list, y_list))\n",
    "    sharpe = combined.mean() / combined.std() * np.sqrt(252*24*4) if combined.std() else 0\n",
    "    acc = accuracy_score(np.concatenate(y_list), np.concatenate(pred_list))\n",
    "    return {'accuracy': acc, 'sharpe': sharpe}\n",
    "\n",
    "# === Main execution pipeline ===\n",
    "def run_pipeline(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    df = df.resample('15min').agg({'open':'first','high':'max','low':'min','close':'last','volume':'sum'}).dropna()\n",
    "\n",
    "    # 1. Generate target labels\n",
    "    target_df = calculate_target_quantile(df, future_bars=3, lower_q=0.2, upper_q=0.8)\n",
    "    df['direction'] = target_df['direction']\n",
    "    df['magnitude'] = target_df['magnitude']\n",
    "    df = df.dropna(subset=['direction'])\n",
    "    print(\"Label distribution:\\n\", df['direction'].value_counts(normalize=True))\n",
    "\n",
    "    # 2. Feature engineering\n",
    "    df_feat = AdvancedFeatureEngineer().fit_transform(df)\n",
    "    df_feat = integrate_candlestick_features(df_feat)\n",
    "\n",
    "    X = df_feat.drop(columns=['open','high','low','close','volume','direction','magnitude'])\n",
    "    y = df_feat['direction']\n",
    "\n",
    "    results, pred_list, y_list, sharpe_list = [], [], [], []\n",
    "    train_len = 5000\n",
    "    test_len  = 1000\n",
    "    step      = 1000\n",
    "\n",
    "    n = len(X)\n",
    "    for start in range(0, n - train_len - test_len + 1, step):\n",
    "        train_idx = range(start, start + train_len)\n",
    "        test_idx  = range(start + train_len, start + train_len + test_len)\n",
    "\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_test,  y_test  = X.iloc[test_idx],  y.iloc[test_idx]\n",
    "\n",
    "        if y_train.nunique() < 2:\n",
    "            print(f\"Window {start}-{start+train_len+test_len} skipped: only {y_train.unique()}\")\n",
    "            continue\n",
    "\n",
    "        X_res, y_res = SMOTE().fit_resample(X_train, y_train)\n",
    "        model = EnhancedMetaModel().fit(X_res, y_res)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        ret = df['close'].iloc[test_idx].pct_change().shift(-1) * np.sign(preds)\n",
    "        sharpe = ret.mean()/ret.std()*np.sqrt(252*24*4) if ret.std() else 0\n",
    "        sharpe_list.append(sharpe)\n",
    "\n",
    "        print(f\"\\n======= Window {start}-{start+train_len-1} → Test {start+train_len}-{start+train_len+test_len-1} =======\")\n",
    "        print(classification_report(y_test, preds, digits=3))\n",
    "        print(f\"Sharpe: {sharpe:.2f} | Acc: {accuracy_score(y_test, preds):.4f}\")\n",
    "\n",
    "        results.append({'accuracy': accuracy_score(y_test, preds), 'sharpe': sharpe})\n",
    "        pred_list.append(preds)\n",
    "        y_list.append(y_test.values)\n",
    "\n",
    "    if not results:\n",
    "        print(\"❌ No valid folds\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n======= Summary =======\")\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"Fold {i}: Acc={r['accuracy']:.4f}, Sharpe={r['sharpe']:.2f}\")\n",
    "\n",
    "    all_ret = np.concatenate([np.sign(y)*0.001*np.sign(p) for p, y in zip(pred_list, y_list)])\n",
    "    print(\"White Reality Check p-value:\", block_bootstrap_pval(all_ret))\n",
    "\n",
    "    w = weighted_signal_evaluation(pred_list, y_list, np.array(sharpe_list))\n",
    "    print(f\"Weighted Acc={w['accuracy']:.4f}, Sharpe={w['sharpe']:.2f}\")\n",
    "\n",
    "    joblib.dump(EnhancedMetaModel().fit(X, y), \"final_model.pkl\")\n",
    "    print(\"Saved final_model.pkl\")\n",
    "\n",
    "    df_feat = AdvancedFeatureEngineer().fit_transform(df)\n",
    "    df_feat = integrate_candlestick_features(df_feat)\n",
    "    df_plot = df_feat.copy()\n",
    "    df_plot['MA20'] = df_plot['close'].rolling(20).mean()\n",
    "    df_plot['MA50'] = df_plot['close'].rolling(50).mean()\n",
    "    df_plot['RSI']  = talib.RSI(df_plot['close'])\n",
    "\n",
    "    patterns = ['BearishEngulfing','ThreeWhiteSoldiers','InvertedHammer']\n",
    "    for pat in patterns:\n",
    "        df_plot[f\"Signal_{pat}\"] = np.where(\n",
    "            df_plot[pat]==100,  1,\n",
    "            np.where(df_plot[pat]==-100, -1, 0)\n",
    "        )\n",
    "\n",
    "    plot_pattern_results(\n",
    "        df=df_plot,\n",
    "        patterns=patterns,\n",
    "        symbol='BTCUSDT',\n",
    "        max_points=2000,\n",
    "        buffer=50,\n",
    "        open_browser=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "009d4ba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T02:59:41.845665Z",
     "start_time": "2025-07-04T02:51:16.998150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      " direction\n",
      " 0    0.600023\n",
      "-1    0.199989\n",
      " 1    0.199989\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "======= Window 0-4999 → Test 5000-5999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.378     0.115     0.176       148\n",
      "           0      0.813     0.769     0.790       731\n",
      "           1      0.212     0.463     0.291       121\n",
      "\n",
      "    accuracy                          0.635      1000\n",
      "   macro avg      0.468     0.449     0.419      1000\n",
      "weighted avg      0.676     0.635     0.639      1000\n",
      "\n",
      "Sharpe: -3.05 | Acc: 0.6350\n",
      "\n",
      "======= Window 1000-5999 → Test 6000-6999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.284     0.216     0.246       259\n",
      "           0      0.608     0.396     0.479       513\n",
      "           1      0.284     0.583     0.382       228\n",
      "\n",
      "    accuracy                          0.392      1000\n",
      "   macro avg      0.392     0.398     0.369      1000\n",
      "weighted avg      0.450     0.392     0.397      1000\n",
      "\n",
      "Sharpe: 6.72 | Acc: 0.3920\n",
      "\n",
      "======= Window 2000-6999 → Test 7000-7999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.206     0.317     0.250       189\n",
      "           0      0.653     0.593     0.622       583\n",
      "           1      0.313     0.246     0.275       228\n",
      "\n",
      "    accuracy                          0.462      1000\n",
      "   macro avg      0.391     0.386     0.382      1000\n",
      "weighted avg      0.491     0.462     0.472      1000\n",
      "\n",
      "Sharpe: 4.91 | Acc: 0.4620\n",
      "\n",
      "======= Window 3000-7999 → Test 8000-8999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.313     0.295     0.303       224\n",
      "           0      0.640     0.684     0.661       563\n",
      "           1      0.294     0.258     0.275       213\n",
      "\n",
      "    accuracy                          0.506      1000\n",
      "   macro avg      0.415     0.412     0.413      1000\n",
      "weighted avg      0.493     0.506     0.499      1000\n",
      "\n",
      "Sharpe: 1.12 | Acc: 0.5060\n",
      "\n",
      "======= Window 4000-8999 → Test 9000-9999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.349     0.170     0.229       311\n",
      "           0      0.646     0.440     0.523       432\n",
      "           1      0.314     0.677     0.429       257\n",
      "\n",
      "    accuracy                          0.417      1000\n",
      "   macro avg      0.436     0.429     0.394      1000\n",
      "weighted avg      0.468     0.417     0.408      1000\n",
      "\n",
      "Sharpe: 2.08 | Acc: 0.4170\n",
      "\n",
      "======= Window 5000-9999 → Test 10000-10999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.262     0.168     0.204       191\n",
      "           0      0.673     0.774     0.720       597\n",
      "           1      0.354     0.321     0.337       212\n",
      "\n",
      "    accuracy                          0.562      1000\n",
      "   macro avg      0.430     0.421     0.420      1000\n",
      "weighted avg      0.527     0.562     0.540      1000\n",
      "\n",
      "Sharpe: -4.34 | Acc: 0.5620\n",
      "\n",
      "======= Window 6000-10999 → Test 11000-11999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.228     0.195     0.210       190\n",
      "           0      0.698     0.788     0.741       623\n",
      "           1      0.348     0.251     0.292       187\n",
      "\n",
      "    accuracy                          0.575      1000\n",
      "   macro avg      0.425     0.411     0.414      1000\n",
      "weighted avg      0.544     0.575     0.556      1000\n",
      "\n",
      "Sharpe: -2.28 | Acc: 0.5750\n",
      "\n",
      "======= Window 7000-11999 → Test 12000-12999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.219     0.160     0.185       219\n",
      "           0      0.620     0.676     0.646       552\n",
      "           1      0.294     0.306     0.300       229\n",
      "\n",
      "    accuracy                          0.478      1000\n",
      "   macro avg      0.377     0.380     0.377      1000\n",
      "weighted avg      0.457     0.478     0.466      1000\n",
      "\n",
      "Sharpe: 8.11 | Acc: 0.4780\n",
      "\n",
      "======= Window 8000-12999 → Test 13000-13999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.244     0.254     0.249       169\n",
      "           0      0.718     0.813     0.763       638\n",
      "           1      0.356     0.187     0.245       193\n",
      "\n",
      "    accuracy                          0.598      1000\n",
      "   macro avg      0.440     0.418     0.419      1000\n",
      "weighted avg      0.568     0.598     0.576      1000\n",
      "\n",
      "Sharpe: -2.13 | Acc: 0.5980\n",
      "\n",
      "======= Window 9000-13999 → Test 14000-14999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.270     0.324     0.294       179\n",
      "           0      0.716     0.806     0.758       660\n",
      "           1      0.286     0.075     0.118       161\n",
      "\n",
      "    accuracy                          0.602      1000\n",
      "   macro avg      0.424     0.402     0.390      1000\n",
      "weighted avg      0.567     0.602     0.572      1000\n",
      "\n",
      "Sharpe: 1.83 | Acc: 0.6020\n",
      "\n",
      "======= Window 10000-14999 → Test 15000-15999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.242     0.264     0.252       148\n",
      "           0      0.762     0.838     0.798       698\n",
      "           1      0.366     0.169     0.231       154\n",
      "\n",
      "    accuracy                          0.650      1000\n",
      "   macro avg      0.457     0.423     0.427      1000\n",
      "weighted avg      0.624     0.650     0.630      1000\n",
      "\n",
      "Sharpe: -6.23 | Acc: 0.6500\n",
      "\n",
      "======= Window 11000-15999 → Test 16000-16999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.257     0.367     0.302       169\n",
      "           0      0.734     0.781     0.757       667\n",
      "           1      0.429     0.128     0.197       164\n",
      "\n",
      "    accuracy                          0.604      1000\n",
      "   macro avg      0.473     0.425     0.419      1000\n",
      "weighted avg      0.603     0.604     0.588      1000\n",
      "\n",
      "Sharpe: -2.84 | Acc: 0.6040\n",
      "\n",
      "======= Window 12000-16999 → Test 17000-17999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.212     0.256     0.232       168\n",
      "           0      0.737     0.765     0.751       668\n",
      "           1      0.308     0.195     0.239       164\n",
      "\n",
      "    accuracy                          0.586      1000\n",
      "   macro avg      0.419     0.405     0.407      1000\n",
      "weighted avg      0.579     0.586     0.580      1000\n",
      "\n",
      "Sharpe: 3.25 | Acc: 0.5860\n",
      "\n",
      "======= Window 13000-17999 → Test 18000-18999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.262     0.720     0.384       232\n",
      "           0      0.763     0.364     0.493       470\n",
      "           1      0.396     0.185     0.252       298\n",
      "\n",
      "    accuracy                          0.393      1000\n",
      "   macro avg      0.474     0.423     0.376      1000\n",
      "weighted avg      0.538     0.393     0.396      1000\n",
      "\n",
      "Sharpe: 2.93 | Acc: 0.3930\n",
      "\n",
      "======= Window 14000-18999 → Test 19000-19999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.205     0.485     0.288       194\n",
      "           0      0.648     0.501     0.565       551\n",
      "           1      0.426     0.192     0.265       255\n",
      "\n",
      "    accuracy                          0.419      1000\n",
      "   macro avg      0.426     0.393     0.373      1000\n",
      "weighted avg      0.505     0.419     0.435      1000\n",
      "\n",
      "Sharpe: -4.07 | Acc: 0.4190\n",
      "\n",
      "======= Window 15000-19999 → Test 20000-20999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.310     0.176     0.225       221\n",
      "           0      0.646     0.617     0.631       553\n",
      "           1      0.283     0.434     0.343       226\n",
      "\n",
      "    accuracy                          0.478      1000\n",
      "   macro avg      0.413     0.409     0.399      1000\n",
      "weighted avg      0.490     0.478     0.476      1000\n",
      "\n",
      "Sharpe: 0.01 | Acc: 0.4780\n",
      "\n",
      "======= Window 16000-20999 → Test 21000-21999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.265     0.223     0.242       193\n",
      "           0      0.689     0.743     0.715       584\n",
      "           1      0.337     0.314     0.325       223\n",
      "\n",
      "    accuracy                          0.547      1000\n",
      "   macro avg      0.430     0.427     0.427      1000\n",
      "weighted avg      0.529     0.547     0.537      1000\n",
      "\n",
      "Sharpe: -0.08 | Acc: 0.5470\n",
      "\n",
      "======= Window 17000-21999 → Test 22000-22999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.360     0.171     0.231       293\n",
      "           0      0.524     0.685     0.594       454\n",
      "           1      0.313     0.332     0.322       253\n",
      "\n",
      "    accuracy                          0.445      1000\n",
      "   macro avg      0.399     0.396     0.383      1000\n",
      "weighted avg      0.423     0.445     0.419      1000\n",
      "\n",
      "Sharpe: -2.27 | Acc: 0.4450\n",
      "\n",
      "======= Window 18000-22999 → Test 23000-23999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.188     0.062     0.094       144\n",
      "           0      0.743     0.928     0.826       711\n",
      "           1      0.344     0.152     0.211       145\n",
      "\n",
      "    accuracy                          0.691      1000\n",
      "   macro avg      0.425     0.381     0.377      1000\n",
      "weighted avg      0.605     0.691     0.631      1000\n",
      "\n",
      "Sharpe: 2.68 | Acc: 0.6910\n",
      "\n",
      "======= Window 19000-23999 → Test 24000-24999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.287     0.144     0.192       201\n",
      "           0      0.695     0.756     0.724       606\n",
      "           1      0.329     0.409     0.365       193\n",
      "\n",
      "    accuracy                          0.566      1000\n",
      "   macro avg      0.437     0.436     0.427      1000\n",
      "weighted avg      0.542     0.566     0.548      1000\n",
      "\n",
      "Sharpe: -1.52 | Acc: 0.5660\n",
      "\n",
      "======= Window 20000-24999 → Test 25000-25999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.277     0.624     0.384       218\n",
      "           0      0.713     0.556     0.625       541\n",
      "           1      0.402     0.145     0.213       241\n",
      "\n",
      "    accuracy                          0.472      1000\n",
      "   macro avg      0.464     0.442     0.407      1000\n",
      "weighted avg      0.543     0.472     0.473      1000\n",
      "\n",
      "Sharpe: -2.12 | Acc: 0.4720\n",
      "\n",
      "======= Window 21000-25999 → Test 26000-26999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.301     0.236     0.265       280\n",
      "           0      0.600     0.614     0.607       492\n",
      "           1      0.327     0.399     0.360       228\n",
      "\n",
      "    accuracy                          0.459      1000\n",
      "   macro avg      0.410     0.416     0.410      1000\n",
      "weighted avg      0.454     0.459     0.455      1000\n",
      "\n",
      "Sharpe: -4.60 | Acc: 0.4590\n",
      "\n",
      "======= Window 22000-26999 → Test 27000-27999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.368     0.093     0.148       151\n",
      "           0      0.770     0.875     0.819       705\n",
      "           1      0.248     0.278     0.262       144\n",
      "\n",
      "    accuracy                          0.671      1000\n",
      "   macro avg      0.462     0.415     0.410      1000\n",
      "weighted avg      0.634     0.671     0.638      1000\n",
      "\n",
      "Sharpe: 2.26 | Acc: 0.6710\n",
      "\n",
      "======= Window 23000-27999 → Test 28000-28999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.375     0.043     0.077       210\n",
      "           0      0.771     0.843     0.805       648\n",
      "           1      0.265     0.500     0.346       142\n",
      "\n",
      "    accuracy                          0.626      1000\n",
      "   macro avg      0.470     0.462     0.410      1000\n",
      "weighted avg      0.616     0.626     0.587      1000\n",
      "\n",
      "Sharpe: -8.58 | Acc: 0.6260\n",
      "\n",
      "======= Window 24000-28999 → Test 29000-29999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.298     0.175     0.220       292\n",
      "           0      0.553     0.498     0.524       430\n",
      "           1      0.346     0.550     0.425       278\n",
      "\n",
      "    accuracy                          0.418      1000\n",
      "   macro avg      0.399     0.408     0.390      1000\n",
      "weighted avg      0.421     0.418     0.408      1000\n",
      "\n",
      "Sharpe: 13.73 | Acc: 0.4180\n",
      "\n",
      "======= Window 25000-29999 → Test 30000-30999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.283     0.119     0.167       236\n",
      "           0      0.625     0.706     0.663       514\n",
      "           1      0.341     0.436     0.382       250\n",
      "\n",
      "    accuracy                          0.500      1000\n",
      "   macro avg      0.416     0.420     0.404      1000\n",
      "weighted avg      0.473     0.500     0.476      1000\n",
      "\n",
      "Sharpe: -6.61 | Acc: 0.5000\n",
      "\n",
      "======= Window 26000-30999 → Test 31000-31999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.238     0.029     0.052       171\n",
      "           0      0.702     0.844     0.766       672\n",
      "           1      0.240     0.261     0.250       157\n",
      "\n",
      "    accuracy                          0.613      1000\n",
      "   macro avg      0.393     0.378     0.356      1000\n",
      "weighted avg      0.550     0.613     0.563      1000\n",
      "\n",
      "Sharpe: -4.85 | Acc: 0.6130\n",
      "\n",
      "======= Window 27000-31999 → Test 32000-32999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.500     0.057     0.103       261\n",
      "           0      0.657     0.573     0.612       494\n",
      "           1      0.330     0.727     0.454       245\n",
      "\n",
      "    accuracy                          0.476      1000\n",
      "   macro avg      0.496     0.452     0.390      1000\n",
      "weighted avg      0.536     0.476     0.440      1000\n",
      "\n",
      "Sharpe: -3.97 | Acc: 0.4760\n",
      "\n",
      "======= Window 28000-32999 → Test 33000-33999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.469     0.131     0.205       175\n",
      "           0      0.717     0.787     0.750       624\n",
      "           1      0.308     0.408     0.351       201\n",
      "\n",
      "    accuracy                          0.596      1000\n",
      "   macro avg      0.498     0.442     0.436      1000\n",
      "weighted avg      0.591     0.596     0.575      1000\n",
      "\n",
      "Sharpe: 7.08 | Acc: 0.5960\n",
      "\n",
      "======= Window 29000-33999 → Test 34000-34999 =======\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.220     0.324     0.262       139\n",
      "           0      0.749     0.853     0.798       693\n",
      "           1      0.000     0.000     0.000       168\n",
      "\n",
      "    accuracy                          0.636      1000\n",
      "   macro avg      0.323     0.392     0.353      1000\n",
      "weighted avg      0.550     0.636     0.589      1000\n",
      "\n",
      "Sharpe: -9.68 | Acc: 0.6360\n",
      "\n",
      "======= Summary =======\n",
      "Fold 1: Acc=0.6350, Sharpe=-3.05\n",
      "Fold 2: Acc=0.3920, Sharpe=6.72\n",
      "Fold 3: Acc=0.4620, Sharpe=4.91\n",
      "Fold 4: Acc=0.5060, Sharpe=1.12\n",
      "Fold 5: Acc=0.4170, Sharpe=2.08\n",
      "Fold 6: Acc=0.5620, Sharpe=-4.34\n",
      "Fold 7: Acc=0.5750, Sharpe=-2.28\n",
      "Fold 8: Acc=0.4780, Sharpe=8.11\n",
      "Fold 9: Acc=0.5980, Sharpe=-2.13\n",
      "Fold 10: Acc=0.6020, Sharpe=1.83\n",
      "Fold 11: Acc=0.6500, Sharpe=-6.23\n",
      "Fold 12: Acc=0.6040, Sharpe=-2.84\n",
      "Fold 13: Acc=0.5860, Sharpe=3.25\n",
      "Fold 14: Acc=0.3930, Sharpe=2.93\n",
      "Fold 15: Acc=0.4190, Sharpe=-4.07\n",
      "Fold 16: Acc=0.4780, Sharpe=0.01\n",
      "Fold 17: Acc=0.5470, Sharpe=-0.08\n",
      "Fold 18: Acc=0.4450, Sharpe=-2.27\n",
      "Fold 19: Acc=0.6910, Sharpe=2.68\n",
      "Fold 20: Acc=0.5660, Sharpe=-1.52\n",
      "Fold 21: Acc=0.4720, Sharpe=-2.12\n",
      "Fold 22: Acc=0.4590, Sharpe=-4.60\n",
      "Fold 23: Acc=0.6710, Sharpe=2.26\n",
      "Fold 24: Acc=0.6260, Sharpe=-8.58\n",
      "Fold 25: Acc=0.4180, Sharpe=13.73\n",
      "Fold 26: Acc=0.5000, Sharpe=-6.61\n",
      "Fold 27: Acc=0.6130, Sharpe=-4.85\n",
      "Fold 28: Acc=0.4760, Sharpe=-3.97\n",
      "Fold 29: Acc=0.5960, Sharpe=7.08\n",
      "Fold 30: Acc=0.6360, Sharpe=-9.68\n",
      "White Reality Check p-value: 0.495\n",
      "Weighted Acc=0.5358, Sharpe=4.65\n",
      "Saved final_model.pkl\n",
      "Saved ../plots/BearishEngulfing_signals.html\n",
      "Saved ../plots/ThreeWhiteSoldiers_signals.html\n",
      "Saved ../plots/InvertedHammer_signals.html\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"../../data/BTCUSDT_1min_2024-05-01_to_2025-05-01.csv\"\n",
    "run_pipeline(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b7414a742d076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T02:59:41.919216Z",
     "start_time": "2025-07-04T02:59:41.914076Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
